# Affine Transformations

It's very common for measures to be parameterized by `Î¼` and `Ïƒ`, for example as in `Normal(Î¼=3, Ïƒ=4)` or `StudentT(Î½=1, Î¼=3, Ïƒ=4)`. In this context, `Î¼` and `Ïƒ` do not always refer to the mean and standard deviation (the `StudentT` above is equivalent to a Cauchy, so both are undefined).

Rather, `Î¼` is a "location parameter", and `Ïƒ` is a "scale parameter". Together these determine an affine transformation

```math
f(z) = Ïƒ z + Î¼
```

Here are below, we'll use ``z`` to represent an "un-transformed" variable, typically coming from a measure like `Normal()` with no location or scale parameters.

Affine transforms are often incorrectly referred to as "linear". Linearity requires ``f(ax + by) = a f(x) + b f(y)`` for scalars ``a`` and ``b``, which only holds for the above ``f`` if ``Î¼=0``.


## Cholesky-based parameterizations

If the "un-transformed" `z` is a scalar, things are relatively simple. But it's important our approach handle the multivariate case as well.

In the literature, it's common for a multivariate normal distribution to be parameterized by a mean `Î¼` and covariance matrix `Î£`. This is mathematically convenient, but can be very awkward from a computational perspective.

While MeasureTheory.jl includes (or will include) a parameterization using `Î£`, we prefer to work in terms of its Cholesky decomposition ``Ïƒ``.

Using "``Ïƒ``" for this may seem strange at first, so we should explain the notation. Let ``Ïƒ`` be a lower-triangular matrix satisfying

```math
Ïƒ Ïƒáµ— = Î£
```

Then given a (multivariate) standard normal ``z``, the covariance matrix of ``Ïƒ z + Î¼`` is

```math
ğ•[Ïƒ z + Î¼] = Î£
```

Comparing to the one dimensional case where

```math
ğ•[Ïƒ z + Î¼] = ÏƒÂ²
```

shows that the lower Cholesky factor of the covariance generalizes the concept of standard deviation, justifying the notation.

## The "Cholesky precision" parameterization

The ``(Î¼,Ïƒ)`` parameterization is especially convenient for random sampling. Any `z ~ Normal()` determines an `x ~ Normal(Î¼,Ïƒ)` through

```math
x = Ïƒ z + Î¼
```

On the other hand, the log-density computation is not quite so simple. Starting with an ``x``, we need to find ``z`` using

```math
z = Ïƒâ»Â¹ (x - Î¼)
```

so the log-density is

```julia
logdensity(d::Normal{(:Î¼,:Ïƒ)}, x) = logdensity(d.Ïƒ \ (x - d.Î¼)) - logdet(d.Ïƒ)
```

Here the `- logdet(Ïƒ)` is the "log absolute Jacobian", required to account for the stretching of the space.

The above requires solving a linear system, which adds some overhead. Even with the convenience of a lower triangular system, it's still not quite a efficient as a multiplication.

In addition to the covariance ``Î£``, it's also common to parameterize a multivariate normal by its _precision matrix_, ``Î© = Î£â»Â¹``. Similarly to our use of ``Ïƒ``, we'll use ``Ï‰`` for the lower Cholesky factor of ``Î©``.

This allows a more efficient log-density,

```julia
logdensity(d::Normal{(:Î¼,:Ï‰)}, x) = logdensity(d.Ï‰ * (x - d.Î¼)) + logdet(d.Ï‰)
```

