# Affine Transformations

It's very common for measures to be parameterized by `ฮผ` and `ฯ`, for example as in `Normal(ฮผ=3, ฯ=4)` or `StudentT(ฮฝ=1, ฮผ=3, ฯ=4)`. In this context, `ฮผ` and `ฯ` do not always refer to the mean and standard deviation (the `StudentT` above is equivalent to a Cauchy, so both are undefined).

Rather, `ฮผ` is a "location parameter", and `ฯ` is a "scale parameter". Together these determine an affine transformation

```math
f(z) = ฯ z + ฮผ
```

Here are below, we'll use ``z`` to represent an "un-transformed" variable, typically coming from a measure like `Normal()` with no location or scale parameters.

Affine transforms are often incorrectly referred to as "linear". Linearity requires ``f(ax + by) = a f(x) + b f(y)`` for scalars ``a`` and ``b``, which only holds for the above ``f`` if ``ฮผ=0``.


## Cholesky-based parameterizations

If the "un-transformed" `z` is a scalar, things are relatively simple. But it's important our approach handle the multivariate case as well.

In the literature, it's common for a multivariate normal distribution to be parameterized by a mean `ฮผ` and covariance matrix `ฮฃ`. This is mathematically convenient, but can be very awkward from a computational perspective.

While MeasureTheory.jl includes (or will include) a parameterization using `ฮฃ`, we prefer to work in terms of its Cholesky decomposition ``ฯ``.

Using "``ฯ``" for this may seem strange at first, so we should explain the notation. Let ``ฯ`` be a lower-triangular matrix satisfying

```math
ฯ ฯแต = ฮฃ
```

Then given a (multivariate) standard normal ``z``, the covariance matrix of ``ฯ z + ฮผ`` is

```math
๐[ฯ z + ฮผ] = ฮฃ
```

Comparing to the one dimensional case where

```math
๐[ฯ z + ฮผ] = ฯยฒ
```

shows that the lower Cholesky factor of the covariance generalizes the concept of standard deviation, justifying the notation.

## The "Cholesky precision" parameterization

The ``(ฮผ,ฯ)`` parameterization is especially convenient for random sampling. Any `z ~ Normal()` determines an `x ~ Normal(ฮผ,ฯ)` through

```math
x = ฯ z + ฮผ
```

On the other hand, the log-density computation is not quite so simple. Starting with an ``x``, we need to find ``z`` using

```math
z = ฯโปยน (x - ฮผ)
```

so the log-density is

```julia
logdensity(d::Normal{(:ฮผ,:ฯ)}, x) = logdensity(d.ฯ \ (x - d.ฮผ)) - logdet(d.ฯ)
```

Here the `- logdet(ฯ)` is the "log absolute Jacobian", required to account for the stretching of the space.

The above requires solving a linear system, which adds some overhead. Even with the convenience of a lower triangular system, it's still not quite a efficient as a multiplication.

In addition to the covariance ``ฮฃ``, it's also common to parameterize a multivariate normal by its _precision matrix_, ``ฮฉ = ฮฃโปยน``. Similarly to our use of ``ฯ``, we'll use ``ฯ`` for the lower Cholesky factor of ``ฮฉ``.

This allows a more efficient log-density,

```julia
logdensity(d::Normal{(:ฮผ,:ฯ)}, x) = logdensity(d.ฯ * (x - d.ฮผ)) + logdet(d.ฯ)
```

## `AffineTransform`

Transforms like ``z โ ฯ z + ฮผ`` and ``z โ ฯ \ z + ฮผ`` can be represented using an `AffineTransform`. For example,

```julia
julia> f = AffineTransform((ฮผ=3.,ฯ=2.))
AffineTransform{(:ฮผ, :ฯ), Tuple{Float64, Float64}}((ฮผ = 3.0, ฯ = 2.0))

julia> f(1.0)
5.0
```

In the scalar case this is relatively simple to invert. But if `ฯ` is a matrix, this would require matrix inversion. Adding to this complication is that lower triangular matrices are not closed under matrix inversion. 

Our multiple parameterizations make it convenient to deal with these issues. The inverse transform of a ``(ฮผ,ฯ)`` transform will be in terms of ``(ฮผ,ฯ)``, and vice-versa. So

```julia
julia> fโปยน = inv(f)
AffineTransform{(:ฮผ, :ฯ), Tuple{Float64, Float64}}((ฮผ = -1.5, ฯ = 2.0))

julia> f(fโปยน(4))
4.0

julia> fโปยน(f(4))
4.0
```