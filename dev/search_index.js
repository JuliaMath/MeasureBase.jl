var documenterSearchIndex = {"docs":
[{"location":"affine/#Affine-Transformations","page":"Affine Transformations","title":"Affine Transformations","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"It's very common for measures to be parameterized by Œº and œÉ, for example as in Normal(Œº=3, œÉ=4) or StudentT(ŒΩ=1, Œº=3, œÉ=4). In this context, Œº and œÉ do not always refer to the mean and standard deviation (the StudentT above is equivalent to a Cauchy, so both are undefined).","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Rather, Œº is a \"location parameter\", and œÉ is a \"scale parameter\". Together these determine an affine transformation","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"f(z) = œÉ z + Œº","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Here are below, we'll use z to represent an \"un-transformed\" variable, typically coming from a measure like Normal() with no location or scale parameters.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Affine transforms are often incorrectly referred to as \"linear\". Linearity requires f(ax + by) = a f(x) + b f(y) for scalars a and b, which only holds for the above f if Œº=0.","category":"page"},{"location":"affine/#Cholesky-based-parameterizations","page":"Affine Transformations","title":"Cholesky-based parameterizations","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"If the \"un-transformed\" z is a scalar, things are relatively simple. But it's important our approach handle the multivariate case as well.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"In the literature, it's common for a multivariate normal distribution to be parameterized by a mean Œº and covariance matrix Œ£. This is mathematically convenient, but can be very awkward from a computational perspective.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"While MeasureTheory.jl includes (or will include) a parameterization using Œ£, we prefer to work in terms of its Cholesky decomposition œÉ.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Using \"œÉ\" for this may seem strange at first, so we should explain the notation. Let œÉ be a lower-triangular matrix satisfying","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"œÉ œÉ·µó = Œ£","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Then given a (multivariate) standard normal z, the covariance matrix of œÉ z + Œº is","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"ùïçœÉ z + Œº = Œ£","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Comparing to the one dimensional case where","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"ùïçœÉ z + Œº = œÉ¬≤","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"shows that the lower Cholesky factor of the covariance generalizes the concept of standard deviation, justifying the notation.","category":"page"},{"location":"affine/#The-\"Cholesky-precision\"-parameterization","page":"Affine Transformations","title":"The \"Cholesky precision\" parameterization","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"The (ŒºœÉ) parameterization is especially convenient for random sampling. Any z ~ Normal() determines an x ~ Normal(Œº,œÉ) through","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"x = œÉ z + Œº","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"On the other hand, the log-density computation is not quite so simple. Starting with an x, we need to find z using","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"z = œÉ¬π (x - Œº)","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"so the log-density is","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"logdensity(d::Normal{(:Œº,:œÉ)}, x) = logdensity(d.œÉ \\ (x - d.Œº)) - logdet(d.œÉ)","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Here the - logdet(œÉ) is the \"log absolute Jacobian\", required to account for the stretching of the space.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"The above requires solving a linear system, which adds some overhead. Even with the convenience of a lower triangular system, it's still not quite a efficient as a multiplication.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"In addition to the covariance Œ£, it's also common to parameterize a multivariate normal by its precision matrix, Œ© = Œ£¬π. Similarly to our use of œÉ, we'll use œâ for the lower Cholesky factor of Œ©.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"This allows a more efficient log-density,","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"logdensity(d::Normal{(:Œº,:œâ)}, x) = logdensity(d.œâ * (x - d.Œº)) + logdet(d.œâ)","category":"page"},{"location":"affine/#AffineTransform","page":"Affine Transformations","title":"AffineTransform","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Transforms like z  œÉ z + Œº and z  œâ  z + Œº can be represented using an AffineTransform. For example,","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"julia> f = AffineTransform((Œº=3.,œÉ=2.))\nAffineTransform{(:Œº, :œÉ), Tuple{Float64, Float64}}((Œº = 3.0, œÉ = 2.0))\n\njulia> f(1.0)\n5.0","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"In the scalar case this is relatively simple to invert. But if œÉ is a matrix, this would require matrix inversion. Adding to this complication is that lower triangular matrices are not closed under matrix inversion. ","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Our multiple parameterizations make it convenient to deal with these issues. The inverse transform of a (ŒºœÉ) transform will be in terms of (Œºœâ), and vice-versa. So","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"julia> f‚Åª¬π = inv(f)\nAffineTransform{(:Œº, :œâ), Tuple{Float64, Float64}}((Œº = -1.5, œâ = 2.0))\n\njulia> f(f‚Åª¬π(4))\n4.0\n\njulia> f‚Åª¬π(f(4))\n4.0","category":"page"},{"location":"affine/#Affine","page":"Affine Transformations","title":"Affine","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Of particular interest (the whole point of all of this, really) is to have a natural way to work with affine transformations of measures. In accordance with the principle of \"common things should have shorter names\", we call this Affine.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"The structure of Affine is relatively simple:","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"struct Affine{N,M,T} <: AbstractMeasure\n    f::AffineTransform{N,T}\n    parent::M\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureBase","category":"page"},{"location":"#MeasureBase","page":"Home","title":"MeasureBase","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MeasureBase.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MeasureBase]","category":"page"},{"location":"#MeasureBase.Density","page":"Home","title":"MeasureBase.Density","text":"struct Density{M,B}\n    Œº::M\n    base::B\nend\n\nFor measures Œº and ŒΩ with Œº‚â™ŒΩ, the density of Œº with respect to ŒΩ (also called the Radon-Nikodym derivative dŒº/dŒΩ) is a function f defined on the support of ŒΩ with the property that for any measurable a ‚äÇ supp(ŒΩ), Œº(a) = ‚à´‚Çê f dŒΩ.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.DensityMeasure","page":"Home","title":"MeasureBase.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.Likelihood","page":"Home","title":"MeasureBase.Likelihood","text":"Likelihood(M<:ParameterizedMeasure, x)\n\n\"Observe\" a value x, yielding a function from the parameters to ‚Ñù.\n\nLikelihoods are most commonly used in conjunction with an existing prior measure to yield a new measure, the posterior. In Bayes's Law, we have\n\nP(Œ∏x)  P(Œ∏) P(xŒ∏)\n\nHere P(Œ∏) is the prior. If we consider P(xŒ∏) as a function on Œ∏, then it is called a likelihood.\n\nSince measures are most commonly manipulated using density and logdensity, it's awkward to commit a (log-)likelihood to using one or the other. To evaluate a Likelihood, we therefore use density or logdensity, depending on the circumstances. In the latter case, it is of course acting as a log-density.\n\nFor example,\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,)}, 2.0)\nLikelihood(Normal{(:Œº,), T} where T, 2.0)\n\njulia> density(‚Ñì, (Œº=2.0,))\n1.0\n\njulia> logdensity(‚Ñì, (Œº=2.0,))\n-0.0\n\nIf, as above, the measure includes the parameter information, we can optionally leave it out of the second argument in the call to density or logdensity. \n\njulia> density(‚Ñì, 2.0)\n1.0\n\njulia> logdensity(‚Ñì, 2.0)\n-0.0\n\nWith several parameters, things work as expected:\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,:œÉ)}, 2.0)\nLikelihood(Normal{(:Œº, :œÉ), T} where T, 2.0)\n\njulia> logdensity(‚Ñì, (Œº=2, œÉ=3))\n-1.0986122886681098\n\njulia> logdensity(‚Ñì, (2,3))\n-1.0986122886681098\n\njulia> logdensity(‚Ñì, [2, 3])\n-1.0986122886681098\n\n\n\nLikelihood(M<:ParameterizedMeasure, constraint::NamedTuple, x)\n\nIn some cases the measure might have several parameters, and we may want the (log-)likelihood with respect to some subset of them. In this case, we can use the three-argument form, where the second argument is a constraint. For example,\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,:œÉ)}, (œÉ=3.0,), 2.0)\nLikelihood(Normal{(:Œº, :œÉ), T} where T, (œÉ = 3.0,), 2.0)\n\nSimilarly to the above, we have\n\njulia> density(‚Ñì, (Œº=2.0,))\n0.3333333333333333\n\njulia> logdensity(‚Ñì, (Œº=2.0,))\n-1.0986122886681098\n\njulia> density(‚Ñì, 2.0)\n0.3333333333333333\n\njulia> logdensity(‚Ñì, 2.0)\n-1.0986122886681098\n\n\n\nFinally, let's return to the expression for Bayes's Law, \n\nP(Œ∏x)  P(Œ∏) P(xŒ∏)\n\nThe product on the right side is computed pointwise. To work with this in MeasureBase, we have a \"pointwise product\" ‚äô, which takes a measure and a likelihood, and returns a new measure, that is, the unnormalized posterior that has density P(Œ∏) P(xŒ∏) with respect to the base measure of the prior.\n\nFor example, say we have\n\nŒº ~ Normal()\nx ~ Normal(Œº,œÉ)\nœÉ = 1\n\nand we observe x=3. We can compute the posterior measure on Œº as\n\njulia> post = Normal() ‚äô Likelihood(Normal{(:Œº, :œÉ)}, (œÉ=1,), 3)\nNormal() ‚äô Likelihood(Normal{(:Œº, :œÉ), T} where T, (œÉ = 1,), 3)\n\njulia> logdensity(post, 2)\n-2.5\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.SuperpositionMeasure","page":"Home","title":"MeasureBase.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling.\n\nThe superposition of two measures Œº and ŒΩ can be more concisely written as Œº + ŒΩ.\n\nSuperposition measures satisfy\n\nbasemeasure(Œº + ŒΩ) == basemeasure(Œº) + basemeasure(ŒΩ)\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.For-Tuple{Any, Vararg{Any, N} where N}","page":"Home","title":"MeasureBase.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do Œª Exponential(Œª) end |> marginals\n3-element mappedarray(MeasureBase.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:Œª,), Tuple{Int64}}:\n Exponential(Œª = 1,)\n Exponential(Œª = 2,)\n Exponential(Œª = 3,)\n\njulia> For(4,3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n4√ó3 mappedarray(MeasureBase.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)  Normal(Œº = 1, œÉ = 2)  Normal(Œº = 1, œÉ = 3)\n Normal(Œº = 2, œÉ = 1)  Normal(Œº = 2, œÉ = 2)  Normal(Œº = 2, œÉ = 3)\n Normal(Œº = 3, œÉ = 1)  Normal(Œº = 3, œÉ = 2)  Normal(Œº = 3, œÉ = 3)\n Normal(Œº = 4, œÉ = 1)  Normal(Œº = 4, œÉ = 2)  Normal(Œº = 4, œÉ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:Œª,), Tuple{Float64}}:\n Exponential(Œª = -0.268256,)\n Exponential(Œª = 1.53044,)\n Exponential(Œª = -1.08839,)\n\njulia> For(1:3, 1:3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n3-element mappedarray((:Œº, :œÉ)->Main.Normal(Œº, œÉ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)\n Normal(Œº = 2, œÉ = 2)\n Normal(Œº = 3, œÉ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:Œº, :œÉ), Tuple{Float64, Float64}}}:\n Normal(Œº = 0.255024, œÉ = 0.570142)\n Normal(Œº = 0.970706, œÉ = 0.0776745)\n Normal(Œº = 0.731491, œÉ = 0.505837)\n Normal(Œº = 0.563112, œÉ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.kernel","page":"Home","title":"MeasureBase.kernel","text":"kernel(f, M)\nkernel((f1, f2, ...), M)\n\nA kernel Œ∫ = kernel(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that Œ∫(x) = M(f(x)...) respective Œ∫(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), Œ∫(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kernel\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.logdensity","page":"Home","title":"MeasureBase.logdensity","text":"logdensity(Œº::AbstractMeasure{X}, x::X)\n\nCompute the logdensity of the measure Œº at the point x. This is the standard way to define logdensity for a new measure. the base measure is implicit here, and is understood to be basemeasure(Œº).\n\nMethods for computing density relative to other measures will be\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.rootmeasure-Tuple{AbstractMeasure}","page":"Home","title":"MeasureBase.rootmeasure","text":"rootmeasure(Œº::AbstractMeasure)\n\nIt's sometimes important to be able to find the fix point of a measure under basemeasure. That is, to start with some measure and apply basemeasure repeatedly until there's no change. That's what this does.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.‚à´-Tuple{Any, AbstractMeasure}","page":"Home","title":"MeasureBase.‚à´","text":"‚à´(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.‚à´exp-Tuple{Any, Any}","page":"Home","title":"MeasureBase.‚à´exp","text":"‚à´exp(f, base::AbstractMeasure; log=false)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.ùíπ-Tuple{AbstractMeasure, AbstractMeasure}","page":"Home","title":"MeasureBase.ùíπ","text":"ùíπ(Œº::AbstractMeasure, base::AbstractMeasure; log=false)\n\nCompute the Radom-Nikodym derivative (or its log, if log=false) of Œº with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.@domain-Tuple{Any, Any}","page":"Home","title":"MeasureBase.@domain","text":"@domain(name, T)\n\nDefines a new singleton struct T, and a value name for building values of that type.\n\nFor example, @domain ‚Ñù RealNumbers is equivalent to\n\nstruct RealNumbers <: AbstractDomain end\n\nexport ‚Ñù\n\n‚Ñù = RealNumbers()\n\nBase.show(io::IO, ::RealNumbers) = print(io, \"‚Ñù\")\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@half-Tuple{Any}","page":"Home","title":"MeasureBase.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist ‚â™ Lebesgue(‚Ñù), create a new measure Halfdist ‚â™ Lebesgue(‚Ñù‚Çä). For example,\n\n@half Normal()\n\ncreates HalfNormal(), and \n\n@half StudentT(ŒΩ)\n\ncreates HalfStudentT(ŒΩ).\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@parameterized-Tuple{Any}","page":"Home","title":"MeasureBase.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,\n\n@parameterized Normal(Œº,œÉ)\n\ndeclares the Normal is a measure with default parameters Œº and œÉ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\n\nKeywordCalls.@kwstruct Normal(Œº,œÉ)\n\nNormal(Œº,œÉ) = Normal((Œº=Œº, œÉ=œÉ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"}]
}
