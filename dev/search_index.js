var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureBase","category":"page"},{"location":"#MeasureBase","page":"Home","title":"MeasureBase","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MeasureBase.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MeasureBase]","category":"page"},{"location":"#MeasureBase.Density","page":"Home","title":"MeasureBase.Density","text":"struct Density{M,B}\n    μ::M\n    base::B\nend\n\nFor measures μ and ν with μ≪ν, the density of μ with respect to ν (also called the Radon-Nikodym derivative dμ/dν) is a function f defined on the support of ν with the property that for any measurable a ⊂ supp(ν), μ(a) = ∫ₐ f dν.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.DensityMeasure","page":"Home","title":"MeasureBase.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.Likelihood","page":"Home","title":"MeasureBase.Likelihood","text":"Likelihood(M<:ParameterizedMeasure, x)\n\n\"Observe\" a value x, yielding a function from the parameters to ℝ.\n\nLikelihoods are most commonly used in conjunction with an existing prior measure to yield a new measure, the posterior. In Bayes's Law, we have\n\nP(θx)  P(θ) P(xθ)\n\nHere P(θ) is the prior. If we consider P(xθ) as a function on θ, then it is called a likelihood.\n\nSince measures are most commonly manipulated using density and logdensity, it's awkward to commit a (log-)likelihood to using one or the other. To evaluate a Likelihood, we therefore use density or logdensity, depending on the circumstances. In the latter case, it is of course acting as a log-density.\n\nFor example,\n\njulia> ℓ = Likelihood(Normal{(:μ,)}, 2.0)\nLikelihood(Normal{(:μ,), T} where T, 2.0)\n\njulia> density(ℓ, (μ=2.0,))\n1.0\n\njulia> logdensity(ℓ, (μ=2.0,))\n-0.0\n\nIf, as above, the measure includes the parameter information, we can optionally leave it out of the second argument in the call to density or logdensity. \n\njulia> density(ℓ, 2.0)\n1.0\n\njulia> logdensity(ℓ, 2.0)\n-0.0\n\nWith several parameters, things work as expected:\n\njulia> ℓ = Likelihood(Normal{(:μ,:σ)}, 2.0)\nLikelihood(Normal{(:μ, :σ), T} where T, 2.0)\n\njulia> logdensity(ℓ, (μ=2, σ=3))\n-1.0986122886681098\n\njulia> logdensity(ℓ, (2,3))\n-1.0986122886681098\n\njulia> logdensity(ℓ, [2, 3])\n-1.0986122886681098\n\n\n\nLikelihood(M<:ParameterizedMeasure, constraint::NamedTuple, x)\n\nIn some cases the measure might have several parameters, and we may want the (log-)likelihood with respect to some subset of them. In this case, we can use the three-argument form, where the second argument is a constraint. For example,\n\njulia> ℓ = Likelihood(Normal{(:μ,:σ)}, (σ=3.0,), 2.0)\nLikelihood(Normal{(:μ, :σ), T} where T, (σ = 3.0,), 2.0)\n\nSimilarly to the above, we have\n\njulia> density(ℓ, (μ=2.0,))\n0.3333333333333333\n\njulia> logdensity(ℓ, (μ=2.0,))\n-1.0986122886681098\n\njulia> density(ℓ, 2.0)\n0.3333333333333333\n\njulia> logdensity(ℓ, 2.0)\n-1.0986122886681098\n\n\n\nFinally, let's return to the expression for Bayes's Law, \n\nP(θx)  P(θ) P(xθ)\n\nThe product on the right side is computed pointwise. To work with this in MeasureBase, we have a \"pointwise product\" ⊙, which takes a measure and a likelihood, and returns a new measure, that is, the unnormalized posterior that has density P(θ) P(xθ) with respect to the base measure of the prior.\n\nFor example, say we have\n\nμ ~ Normal()\nx ~ Normal(μ,σ)\nσ = 1\n\nand we observe x=3. We can compute the posterior measure on μ as\n\njulia> post = Normal() ⊙ Likelihood(Normal{(:μ, :σ)}, (σ=1,), 3)\nNormal() ⊙ Likelihood(Normal{(:μ, :σ), T} where T, (σ = 1,), 3)\n\njulia> logdensity(post, 2)\n-2.5\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.SuperpositionMeasure","page":"Home","title":"MeasureBase.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling.\n\nThe superposition of two measures μ and ν can be more concisely written as μ + ν.\n\nSuperposition measures satisfy\n\nbasemeasure(μ + ν) == basemeasure(μ) + basemeasure(ν)\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.For-Tuple{Any, Vararg{Any, N} where N}","page":"Home","title":"MeasureBase.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do λ Exponential(λ) end |> marginals\n3-element mappedarray(MeasureBase.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:λ,), Tuple{Int64}}:\n Exponential(λ = 1,)\n Exponential(λ = 2,)\n Exponential(λ = 3,)\n\njulia> For(4,3) do μ,σ Normal(μ,σ) end |> marginals\n4×3 mappedarray(MeasureBase.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:μ, :σ), Tuple{Int64, Int64}}:\n Normal(μ = 1, σ = 1)  Normal(μ = 1, σ = 2)  Normal(μ = 1, σ = 3)\n Normal(μ = 2, σ = 1)  Normal(μ = 2, σ = 2)  Normal(μ = 2, σ = 3)\n Normal(μ = 3, σ = 1)  Normal(μ = 3, σ = 2)  Normal(μ = 3, σ = 3)\n Normal(μ = 4, σ = 1)  Normal(μ = 4, σ = 2)  Normal(μ = 4, σ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:λ,), Tuple{Float64}}:\n Exponential(λ = -0.268256,)\n Exponential(λ = 1.53044,)\n Exponential(λ = -1.08839,)\n\njulia> For(1:3, 1:3) do μ,σ Normal(μ,σ) end |> marginals\n3-element mappedarray((:μ, :σ)->Main.Normal(μ, σ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:μ, :σ), Tuple{Int64, Int64}}:\n Normal(μ = 1, σ = 1)\n Normal(μ = 2, σ = 2)\n Normal(μ = 3, σ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:μ, :σ), Tuple{Float64, Float64}}}:\n Normal(μ = 0.255024, σ = 0.570142)\n Normal(μ = 0.970706, σ = 0.0776745)\n Normal(μ = 0.731491, σ = 0.505837)\n Normal(μ = 0.563112, σ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.kernel","page":"Home","title":"MeasureBase.kernel","text":"kernel(f, M)\nkernel((f1, f2, ...), M)\n\nA kernel κ = kernel(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that κ(x) = M(f(x)...) respective κ(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), κ(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kernel\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.logdensity","page":"Home","title":"MeasureBase.logdensity","text":"logdensity(μ::AbstractMeasure{X}, x::X)\n\nCompute the logdensity of the measure μ at the point x. This is the standard way to define logdensity for a new measure. the base measure is implicit here, and is understood to be basemeasure(μ).\n\nMethods for computing density relative to other measures will be\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.rootmeasure-Tuple{AbstractMeasure}","page":"Home","title":"MeasureBase.rootmeasure","text":"rootmeasure(μ::AbstractMeasure)\n\nIt's sometimes important to be able to find the fix point of a measure under basemeasure. That is, to start with some measure and apply basemeasure repeatedly until there's no change. That's what this does.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.∫-Tuple{Any, AbstractMeasure}","page":"Home","title":"MeasureBase.∫","text":"∫(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.∫exp-Tuple{Any, Any}","page":"Home","title":"MeasureBase.∫exp","text":"∫exp(f, base::AbstractMeasure; log=false)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.𝒹-Tuple{AbstractMeasure, AbstractMeasure}","page":"Home","title":"MeasureBase.𝒹","text":"𝒹(μ::AbstractMeasure, base::AbstractMeasure; log=false)\n\nCompute the Radom-Nikodym derivative (or its log, if log=false) of μ with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.@domain-Tuple{Any, Any}","page":"Home","title":"MeasureBase.@domain","text":"@domain(name, T)\n\nDefines a new singleton struct T, and a value name for building values of that type.\n\nFor example, @domain ℝ RealNumbers is equivalent to\n\nstruct RealNumbers <: AbstractDomain end\n\nexport ℝ\n\nℝ = RealNumbers()\n\nBase.show(io::IO, ::RealNumbers) = print(io, \"ℝ\")\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@half-Tuple{Any}","page":"Home","title":"MeasureBase.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist ≪ Lebesgue(ℝ), create a new measure Halfdist ≪ Lebesgue(ℝ₊). For example,\n\n@half Normal()\n\ncreates HalfNormal(), and \n\n@half StudentT(ν)\n\ncreates HalfStudentT(ν).\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@parameterized-Tuple{Any}","page":"Home","title":"MeasureBase.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,\n\n@parameterized Normal(μ,σ)\n\ndeclares the Normal is a measure with default parameters μ and σ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\n\nKeywordCalls.@kwstruct Normal(μ,σ)\n\nNormal(μ,σ) = Normal((μ=μ, σ=σ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"}]
}
