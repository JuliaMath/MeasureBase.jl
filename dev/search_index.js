var documenterSearchIndex = {"docs":
[{"location":"affine/#Affine-Transformations","page":"Affine Transformations","title":"Affine Transformations","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"It's very common for measures to be parameterized by Œº and œÉ, for example as in Normal(Œº=3,œÉ=4) or StudentT(ŒΩ=1,Œº=3,œÉ=4). In this context, Œº and œÉ do not always refer to the mean and standard deviation (the StudentT above is equivalent to a Cauchy, so both are undefined).","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Rather, Œº is a \"location parameter\", and œÉ is a \"scale parameter\". Together, these determine a transform ","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"x  œÉx + Œº","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureBase","category":"page"},{"location":"#MeasureBase","page":"Home","title":"MeasureBase","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MeasureBase.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MeasureBase]","category":"page"},{"location":"#MeasureBase.Density","page":"Home","title":"MeasureBase.Density","text":"struct Density{M,B}\n    Œº::M\n    base::B\nend\n\nFor measures Œº and ŒΩ with Œº‚â™ŒΩ, the density of Œº with respect to ŒΩ (also called the Radon-Nikodym derivative dŒº/dŒΩ) is a function f defined on the support of ŒΩ with the property that for any measurable a ‚äÇ supp(ŒΩ), Œº(a) = ‚à´‚Çê f dŒΩ.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.DensityMeasure","page":"Home","title":"MeasureBase.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.Likelihood","page":"Home","title":"MeasureBase.Likelihood","text":"Likelihood(M<:ParameterizedMeasure, x)\n\n\"Observe\" a value x, yielding a function from the parameters to ‚Ñù.\n\nLikelihoods are most commonly used in conjunction with an existing prior measure to yield a new measure, the posterior. In Bayes's Law, we have\n\nP(Œ∏x)  P(Œ∏) P(xŒ∏)\n\nHere P(Œ∏) is the prior. If we consider P(xŒ∏) as a function on Œ∏, then it is called a likelihood.\n\nSince measures are most commonly manipulated using density and logdensity, it's awkward to commit a (log-)likelihood to using one or the other. To evaluate a Likelihood, we therefore use density or logdensity, depending on the circumstances. In the latter case, it is of course acting as a log-density.\n\nFor example,\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,)}, 2.0)\nLikelihood(Normal{(:Œº,), T} where T, 2.0)\n\njulia> density(‚Ñì, (Œº=2.0,))\n1.0\n\njulia> logdensity(‚Ñì, (Œº=2.0,))\n-0.0\n\nIf, as above, the measure includes the parameter information, we can optionally leave it out of the second argument in the call to density or logdensity. \n\njulia> density(‚Ñì, 2.0)\n1.0\n\njulia> logdensity(‚Ñì, 2.0)\n-0.0\n\nWith several parameters, things work as expected:\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,:œÉ)}, 2.0)\nLikelihood(Normal{(:Œº, :œÉ), T} where T, 2.0)\n\njulia> logdensity(‚Ñì, (Œº=2, œÉ=3))\n-1.0986122886681098\n\njulia> logdensity(‚Ñì, (2,3))\n-1.0986122886681098\n\njulia> logdensity(‚Ñì, [2, 3])\n-1.0986122886681098\n\n\n\nLikelihood(M<:ParameterizedMeasure, constraint::NamedTuple, x)\n\nIn some cases the measure might have several parameters, and we may want the (log-)likelihood with respect to some subset of them. In this case, we can use the three-argument form, where the second argument is a constraint. For example,\n\njulia> ‚Ñì = Likelihood(Normal{(:Œº,:œÉ)}, (œÉ=3.0,), 2.0)\nLikelihood(Normal{(:Œº, :œÉ), T} where T, (œÉ = 3.0,), 2.0)\n\nSimilarly to the above, we have\n\njulia> density(‚Ñì, (Œº=2.0,))\n0.3333333333333333\n\njulia> logdensity(‚Ñì, (Œº=2.0,))\n-1.0986122886681098\n\njulia> density(‚Ñì, 2.0)\n0.3333333333333333\n\njulia> logdensity(‚Ñì, 2.0)\n-1.0986122886681098\n\n\n\nFinally, let's return to the expression for Bayes's Law, \n\nP(Œ∏x)  P(Œ∏) P(xŒ∏)\n\nThe product on the right side is computed pointwise. To work with this in MeasureBase, we have a \"pointwise product\" ‚äô, which takes a measure and a likelihood, and returns a new measure, that is, the unnormalized posterior that has density P(Œ∏) P(xŒ∏) with respect to the base measure of the prior.\n\nFor example, say we have\n\nŒº ~ Normal()\nx ~ Normal(Œº,œÉ)\nœÉ = 1\n\nand we observe x=3. We can compute the posterior measure on Œº as\n\njulia> post = Normal() ‚äô Likelihood(Normal{(:Œº, :œÉ)}, (œÉ=1,), 3)\nNormal() ‚äô Likelihood(Normal{(:Œº, :œÉ), T} where T, (œÉ = 1,), 3)\n\njulia> logdensity(post, 2)\n-2.5\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.SuperpositionMeasure","page":"Home","title":"MeasureBase.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling.\n\nThe superposition of two measures Œº and ŒΩ can be more concisely written as Œº + ŒΩ.\n\nSuperposition measures satisfy\n\nbasemeasure(Œº + ŒΩ) == basemeasure(Œº) + basemeasure(ŒΩ)\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.For-Tuple{Any, Vararg{Any, N} where N}","page":"Home","title":"MeasureBase.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do Œª Exponential(Œª) end |> marginals\n3-element mappedarray(MeasureBase.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:Œª,), Tuple{Int64}}:\n Exponential(Œª = 1,)\n Exponential(Œª = 2,)\n Exponential(Œª = 3,)\n\njulia> For(4,3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n4√ó3 mappedarray(MeasureBase.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)  Normal(Œº = 1, œÉ = 2)  Normal(Œº = 1, œÉ = 3)\n Normal(Œº = 2, œÉ = 1)  Normal(Œº = 2, œÉ = 2)  Normal(Œº = 2, œÉ = 3)\n Normal(Œº = 3, œÉ = 1)  Normal(Œº = 3, œÉ = 2)  Normal(Œº = 3, œÉ = 3)\n Normal(Œº = 4, œÉ = 1)  Normal(Œº = 4, œÉ = 2)  Normal(Œº = 4, œÉ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:Œª,), Tuple{Float64}}:\n Exponential(Œª = -0.268256,)\n Exponential(Œª = 1.53044,)\n Exponential(Œª = -1.08839,)\n\njulia> For(1:3, 1:3) do Œº,œÉ Normal(Œº,œÉ) end |> marginals\n3-element mappedarray((:Œº, :œÉ)->Main.Normal(Œº, œÉ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:Œº, :œÉ), Tuple{Int64, Int64}}:\n Normal(Œº = 1, œÉ = 1)\n Normal(Œº = 2, œÉ = 2)\n Normal(Œº = 3, œÉ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:Œº, :œÉ), Tuple{Float64, Float64}}}:\n Normal(Œº = 0.255024, œÉ = 0.570142)\n Normal(Œº = 0.970706, œÉ = 0.0776745)\n Normal(Œº = 0.731491, œÉ = 0.505837)\n Normal(Œº = 0.563112, œÉ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.kernel","page":"Home","title":"MeasureBase.kernel","text":"kernel(f, M)\nkernel((f1, f2, ...), M)\n\nA kernel Œ∫ = kernel(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that Œ∫(x) = M(f(x)...) respective Œ∫(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), Œ∫(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kernel\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.logdensity","page":"Home","title":"MeasureBase.logdensity","text":"logdensity(Œº::AbstractMeasure{X}, x::X)\n\nCompute the logdensity of the measure Œº at the point x. This is the standard way to define logdensity for a new measure. the base measure is implicit here, and is understood to be basemeasure(Œº).\n\nMethods for computing density relative to other measures will be\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.rootmeasure-Tuple{AbstractMeasure}","page":"Home","title":"MeasureBase.rootmeasure","text":"rootmeasure(Œº::AbstractMeasure)\n\nIt's sometimes important to be able to find the fix point of a measure under basemeasure. That is, to start with some measure and apply basemeasure repeatedly until there's no change. That's what this does.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.‚à´-Tuple{Any, AbstractMeasure}","page":"Home","title":"MeasureBase.‚à´","text":"‚à´(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.‚à´exp-Tuple{Any, Any}","page":"Home","title":"MeasureBase.‚à´exp","text":"‚à´exp(f, base::AbstractMeasure; log=false)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.ùíπ-Tuple{AbstractMeasure, AbstractMeasure}","page":"Home","title":"MeasureBase.ùíπ","text":"ùíπ(Œº::AbstractMeasure, base::AbstractMeasure; log=false)\n\nCompute the Radom-Nikodym derivative (or its log, if log=false) of Œº with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.@domain-Tuple{Any, Any}","page":"Home","title":"MeasureBase.@domain","text":"@domain(name, T)\n\nDefines a new singleton struct T, and a value name for building values of that type.\n\nFor example, @domain ‚Ñù RealNumbers is equivalent to\n\nstruct RealNumbers <: AbstractDomain end\n\nexport ‚Ñù\n\n‚Ñù = RealNumbers()\n\nBase.show(io::IO, ::RealNumbers) = print(io, \"‚Ñù\")\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@half-Tuple{Any}","page":"Home","title":"MeasureBase.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist ‚â™ Lebesgue(‚Ñù), create a new measure Halfdist ‚â™ Lebesgue(‚Ñù‚Çä). For example,\n\n@half Normal()\n\ncreates HalfNormal(), and \n\n@half StudentT(ŒΩ)\n\ncreates HalfStudentT(ŒΩ).\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@parameterized-Tuple{Any}","page":"Home","title":"MeasureBase.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,\n\n@parameterized Normal(Œº,œÉ)\n\ndeclares the Normal is a measure with default parameters Œº and œÉ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\n\nKeywordCalls.@kwstruct Normal(Œº,œÉ)\n\nNormal(Œº,œÉ) = Normal((Œº=Œº, œÉ=œÉ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"}]
}
