var documenterSearchIndex = {"docs":
[{"location":"affine/#Affine-Transformations","page":"Affine Transformations","title":"Affine Transformations","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"It's very common for measures to be parameterized by Î¼ and Ïƒ, for example as in Normal(Î¼=3, Ïƒ=4) or StudentT(Î½=1, Î¼=3, Ïƒ=4). In this context, Î¼ and Ïƒ do not always refer to the mean and standard deviation (the StudentT above is equivalent to a Cauchy, so both are undefined).","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Rather, Î¼ is a \"location parameter\", and Ïƒ is a \"scale parameter\". Together these determine an affine transformation","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"f(z) = Ïƒ z + Î¼","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Here are below, we'll use z to represent an \"un-transformed\" variable, typically coming from a measure like Normal() with no location or scale parameters.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Affine transforms are often incorrectly referred to as \"linear\". Linearity requires f(ax + by) = a f(x) + b f(y) for scalars a and b, which only holds for the above f if Î¼=0.","category":"page"},{"location":"affine/#Cholesky-based-parameterizations","page":"Affine Transformations","title":"Cholesky-based parameterizations","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"If the \"un-transformed\" z is a scalar, things are relatively simple. But it's important our approach handle the multivariate case as well.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"In the literature, it's common for a multivariate normal distribution to be parameterized by a mean Î¼ and covariance matrix Î£. This is mathematically convenient, but can be very awkward from a computational perspective.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"While MeasureTheory.jl includes (or will include) a parameterization using Î£, we prefer to work in terms of its Cholesky decomposition Ïƒ.","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Using \"Ïƒ\" for this may seem strange at first, so we should explain the notation. Let Ïƒ be a lower-triangular matrix satisfying","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Ïƒ Ïƒáµ— = Î£","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"Then given a (multivariate) standard normal z, the covariance matrix of Ïƒ z + Î¼ is","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"ğ•Ïƒ z + Î¼ = Î£","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"This is similar to the one dimensional case where","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"ğ•Ïƒ z + Î¼ = ÏƒÂ² ","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"and so the lower Cholesky factor of the covariance generalizes the concept of standard deviation, justifying the notation.","category":"page"},{"location":"affine/#Affine-and-AffineTransform","page":"Affine Transformations","title":"Affine and AffineTransform","text":"","category":"section"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"unif = âˆ«(x -> 0<x<1, Lebesgue(â„))     f = AffineTransform((Î¼=3,Ïƒ=2))     g = AffineTransform((Î¼=3,Ï‰=2))","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"So for example, the implementation of StudentT(Î½=1, Î¼=3, Ïƒ=4) is equivalent to","category":"page"},{"location":"affine/","page":"Affine Transformations","title":"Affine Transformations","text":"StudentT(nt::NamedTuple{(:Î½,:Î¼,:Ïƒ)}) = Affine((Î¼=nt.Î¼, Ïƒ=nt.Ïƒ), StudentT((Î½=1)))","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MeasureBase","category":"page"},{"location":"#MeasureBase","page":"Home","title":"MeasureBase","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MeasureBase.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MeasureBase]","category":"page"},{"location":"#MeasureBase.Density","page":"Home","title":"MeasureBase.Density","text":"struct Density{M,B}\n    Î¼::M\n    base::B\nend\n\nFor measures Î¼ and Î½ with Î¼â‰ªÎ½, the density of Î¼ with respect to Î½ (also called the Radon-Nikodym derivative dÎ¼/dÎ½) is a function f defined on the support of Î½ with the property that for any measurable a âŠ‚ supp(Î½), Î¼(a) = âˆ«â‚ f dÎ½.\n\nBecause this function is often difficult to express in closed form, there are many different ways of computing it. We therefore provide a formal representation to allow comptuational flexibilty.\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.DensityMeasure","page":"Home","title":"MeasureBase.DensityMeasure","text":"struct DensityMeasure{F,B} <: AbstractMeasure\n    density :: F\n    base    :: B\nend\n\nA DensityMeasure is a measure defined by a density with respect to some other \"base\" measure \n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.Likelihood","page":"Home","title":"MeasureBase.Likelihood","text":"Likelihood(M<:ParameterizedMeasure, x)\n\n\"Observe\" a value x, yielding a function from the parameters to â„.\n\nLikelihoods are most commonly used in conjunction with an existing prior measure to yield a new measure, the posterior. In Bayes's Law, we have\n\nP(Î¸x)  P(Î¸) P(xÎ¸)\n\nHere P(Î¸) is the prior. If we consider P(xÎ¸) as a function on Î¸, then it is called a likelihood.\n\nSince measures are most commonly manipulated using density and logdensity, it's awkward to commit a (log-)likelihood to using one or the other. To evaluate a Likelihood, we therefore use density or logdensity, depending on the circumstances. In the latter case, it is of course acting as a log-density.\n\nFor example,\n\njulia> â„“ = Likelihood(Normal{(:Î¼,)}, 2.0)\nLikelihood(Normal{(:Î¼,), T} where T, 2.0)\n\njulia> density(â„“, (Î¼=2.0,))\n1.0\n\njulia> logdensity(â„“, (Î¼=2.0,))\n-0.0\n\nIf, as above, the measure includes the parameter information, we can optionally leave it out of the second argument in the call to density or logdensity. \n\njulia> density(â„“, 2.0)\n1.0\n\njulia> logdensity(â„“, 2.0)\n-0.0\n\nWith several parameters, things work as expected:\n\njulia> â„“ = Likelihood(Normal{(:Î¼,:Ïƒ)}, 2.0)\nLikelihood(Normal{(:Î¼, :Ïƒ), T} where T, 2.0)\n\njulia> logdensity(â„“, (Î¼=2, Ïƒ=3))\n-1.0986122886681098\n\njulia> logdensity(â„“, (2,3))\n-1.0986122886681098\n\njulia> logdensity(â„“, [2, 3])\n-1.0986122886681098\n\n\n\nLikelihood(M<:ParameterizedMeasure, constraint::NamedTuple, x)\n\nIn some cases the measure might have several parameters, and we may want the (log-)likelihood with respect to some subset of them. In this case, we can use the three-argument form, where the second argument is a constraint. For example,\n\njulia> â„“ = Likelihood(Normal{(:Î¼,:Ïƒ)}, (Ïƒ=3.0,), 2.0)\nLikelihood(Normal{(:Î¼, :Ïƒ), T} where T, (Ïƒ = 3.0,), 2.0)\n\nSimilarly to the above, we have\n\njulia> density(â„“, (Î¼=2.0,))\n0.3333333333333333\n\njulia> logdensity(â„“, (Î¼=2.0,))\n-1.0986122886681098\n\njulia> density(â„“, 2.0)\n0.3333333333333333\n\njulia> logdensity(â„“, 2.0)\n-1.0986122886681098\n\n\n\nFinally, let's return to the expression for Bayes's Law, \n\nP(Î¸x)  P(Î¸) P(xÎ¸)\n\nThe product on the right side is computed pointwise. To work with this in MeasureBase, we have a \"pointwise product\" âŠ™, which takes a measure and a likelihood, and returns a new measure, that is, the unnormalized posterior that has density P(Î¸) P(xÎ¸) with respect to the base measure of the prior.\n\nFor example, say we have\n\nÎ¼ ~ Normal()\nx ~ Normal(Î¼,Ïƒ)\nÏƒ = 1\n\nand we observe x=3. We can compute the posterior measure on Î¼ as\n\njulia> post = Normal() âŠ™ Likelihood(Normal{(:Î¼, :Ïƒ)}, (Ïƒ=1,), 3)\nNormal() âŠ™ Likelihood(Normal{(:Î¼, :Ïƒ), T} where T, (Ïƒ = 1,), 3)\n\njulia> logdensity(post, 2)\n-2.5\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.SuperpositionMeasure","page":"Home","title":"MeasureBase.SuperpositionMeasure","text":"struct SuperpositionMeasure{X,NT} <: AbstractMeasure\n    components :: NT\nend\n\nSuperposition of measures is analogous to mixture distributions, but (because measures need not be normalized) requires no scaling.\n\nThe superposition of two measures Î¼ and Î½ can be more concisely written as Î¼ + Î½.\n\nSuperposition measures satisfy\n\nbasemeasure(Î¼ + Î½) == basemeasure(Î¼) + basemeasure(Î½)\n\n\n\n\n\n","category":"type"},{"location":"#MeasureBase.For-Tuple{Any, Vararg{Any, N} where N}","page":"Home","title":"MeasureBase.For","text":"For(f, base...)\n\nFor provides a convenient way to construct a ProductMeasure. There are several options for the base. With Julia's do notation, this can look very similar to a standard for loop, while maintaining semantics structure that's easier to work with.\n\n\n\nFor(f, base::Int...)\n\nWhen one or several Int values are passed for base, the result is treated as depending on CartesianIndices(base). \n\njulia> For(3) do Î» Exponential(Î») end |> marginals\n3-element mappedarray(MeasureBase.var\"#17#18\"{var\"#15#16\"}(var\"#15#16\"()), ::CartesianIndices{1, Tuple{Base.OneTo{Int64}}}) with eltype Exponential{(:Î»,), Tuple{Int64}}:\n Exponential(Î» = 1,)\n Exponential(Î» = 2,)\n Exponential(Î» = 3,)\n\njulia> For(4,3) do Î¼,Ïƒ Normal(Î¼,Ïƒ) end |> marginals\n4Ã—3 mappedarray(MeasureBase.var\"#17#18\"{var\"#11#12\"}(var\"#11#12\"()), ::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}) with eltype Normal{(:Î¼, :Ïƒ), Tuple{Int64, Int64}}:\n Normal(Î¼ = 1, Ïƒ = 1)  Normal(Î¼ = 1, Ïƒ = 2)  Normal(Î¼ = 1, Ïƒ = 3)\n Normal(Î¼ = 2, Ïƒ = 1)  Normal(Î¼ = 2, Ïƒ = 2)  Normal(Î¼ = 2, Ïƒ = 3)\n Normal(Î¼ = 3, Ïƒ = 1)  Normal(Î¼ = 3, Ïƒ = 2)  Normal(Î¼ = 3, Ïƒ = 3)\n Normal(Î¼ = 4, Ïƒ = 1)  Normal(Î¼ = 4, Ïƒ = 2)  Normal(Î¼ = 4, Ïƒ = 3)\n\n\n\nFor(f, base::AbstractArray...)`\n\nIn this case, base behaves as if the arrays are zipped together before applying the map.\n\njulia> For(randn(3)) do x Exponential(x) end |> marginals\n3-element mappedarray(x->Main.Exponential(x), ::Vector{Float64}) with eltype Exponential{(:Î»,), Tuple{Float64}}:\n Exponential(Î» = -0.268256,)\n Exponential(Î» = 1.53044,)\n Exponential(Î» = -1.08839,)\n\njulia> For(1:3, 1:3) do Î¼,Ïƒ Normal(Î¼,Ïƒ) end |> marginals\n3-element mappedarray((:Î¼, :Ïƒ)->Main.Normal(Î¼, Ïƒ), ::UnitRange{Int64}, ::UnitRange{Int64}) with eltype Normal{(:Î¼, :Ïƒ), Tuple{Int64, Int64}}:\n Normal(Î¼ = 1, Ïƒ = 1)\n Normal(Î¼ = 2, Ïƒ = 2)\n Normal(Î¼ = 3, Ïƒ = 3)\n\n\n\nFor(f, base::Base.Generator)\n\nFor Generators, the function maps over the values of the generator:\n\njulia> For(eachrow(rand(4,2))) do x Normal(x[1], x[2]) end |> marginals |> collect\n4-element Vector{Normal{(:Î¼, :Ïƒ), Tuple{Float64, Float64}}}:\n Normal(Î¼ = 0.255024, Ïƒ = 0.570142)\n Normal(Î¼ = 0.970706, Ïƒ = 0.0776745)\n Normal(Î¼ = 0.731491, Ïƒ = 0.505837)\n Normal(Î¼ = 0.563112, Ïƒ = 0.98307)\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.kernel","page":"Home","title":"MeasureBase.kernel","text":"kernel(f, M)\nkernel((f1, f2, ...), M)\n\nA kernel Îº = kernel(f, m) returns a wrapper around a function f giving the parameters for a measure of type M, such that Îº(x) = M(f(x)...) respective Îº(x) = M(f1(x), f2(x), ...)\n\nIf the argument is a named tuple (;a=f1, b=f1), Îº(x) is defined as M(;a=f(x),b=g(x)).\n\nReference\n\nhttps://en.wikipedia.org/wiki/Markov_kernel\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.logdensity","page":"Home","title":"MeasureBase.logdensity","text":"logdensity(Î¼::AbstractMeasure{X}, x::X)\n\nCompute the logdensity of the measure Î¼ at the point x. This is the standard way to define logdensity for a new measure. the base measure is implicit here, and is understood to be basemeasure(Î¼).\n\nMethods for computing density relative to other measures will be\n\n\n\n\n\n","category":"function"},{"location":"#MeasureBase.rootmeasure-Tuple{AbstractMeasure}","page":"Home","title":"MeasureBase.rootmeasure","text":"rootmeasure(Î¼::AbstractMeasure)\n\nIt's sometimes important to be able to find the fix point of a measure under basemeasure. That is, to start with some measure and apply basemeasure repeatedly until there's no change. That's what this does.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.âˆ«-Tuple{Any, AbstractMeasure}","page":"Home","title":"MeasureBase.âˆ«","text":"âˆ«(f, base::AbstractMeasure)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.âˆ«exp-Tuple{Any, Any}","page":"Home","title":"MeasureBase.âˆ«exp","text":"âˆ«exp(f, base::AbstractMeasure; log=false)\n\nDefine a new measure in terms of a density f over some measure base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.ğ’¹-Tuple{AbstractMeasure, AbstractMeasure}","page":"Home","title":"MeasureBase.ğ’¹","text":"ğ’¹(Î¼::AbstractMeasure, base::AbstractMeasure; log=false)\n\nCompute the Radom-Nikodym derivative (or its log, if log=false) of Î¼ with respect to base.\n\n\n\n\n\n","category":"method"},{"location":"#MeasureBase.@domain-Tuple{Any, Any}","page":"Home","title":"MeasureBase.@domain","text":"@domain(name, T)\n\nDefines a new singleton struct T, and a value name for building values of that type.\n\nFor example, @domain â„ RealNumbers is equivalent to\n\nstruct RealNumbers <: AbstractDomain end\n\nexport â„\n\nâ„ = RealNumbers()\n\nBase.show(io::IO, ::RealNumbers) = print(io, \"â„\")\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@half-Tuple{Any}","page":"Home","title":"MeasureBase.@half","text":"@half dist([paramnames])\n\nStarting from a symmetric univariate measure dist â‰ª Lebesgue(â„), create a new measure Halfdist â‰ª Lebesgue(â„â‚Š). For example,\n\n@half Normal()\n\ncreates HalfNormal(), and \n\n@half StudentT(Î½)\n\ncreates HalfStudentT(Î½).\n\n\n\n\n\n","category":"macro"},{"location":"#MeasureBase.@parameterized-Tuple{Any}","page":"Home","title":"MeasureBase.@parameterized","text":"@parameterized <declaration>\n\nThe <declaration> gives a measure and its default parameters, and specifies its relation to its base measure. For example,\n\n@parameterized Normal(Î¼,Ïƒ)\n\ndeclares the Normal is a measure with default parameters Î¼ and Ïƒ. The result is equivalent to\n\nstruct Normal{N,T} <: ParameterizedMeasure{N}\n    par :: NamedTuple{N,T}\nend\n\nKeywordCalls.@kwstruct Normal(Î¼,Ïƒ)\n\nNormal(Î¼,Ïƒ) = Normal((Î¼=Î¼, Ïƒ=Ïƒ))\n\nSee KeywordCalls.jl for details on @kwstruct.\n\n\n\n\n\n","category":"macro"}]
}
